import pandas as pd
import numpy as np
import os
import sys

# ‚úÖ IMPORTACI√ìN ROBUSTA
try:
    from .feature_calculator import add_all_features, verify_optuna_compatibility, test_optuna_compatibility
except ImportError:
    # Fallback para ejecuci√≥n directa
    try:
        from feature_calculator import add_all_features, verify_optuna_compatibility, test_optuna_compatibility
    except ImportError:
        print("‚ùå No se puede importar feature_calculator")
        sys.exit(1)

def generate_target(df, forward_candles=20, risk_reward_ratio=1.5, atr_multiplier=1.2):
    """
    ‚úÖ GENERADOR DE TARGET MEJORADO Y CONSERVADOR
    Genera la columna 'target' sin sesgo de anticipaci√≥n (lookahead bias)
    
    Args:
        df: DataFrame con datos OHLC y ATR
        forward_candles: Velas futuras a evaluar (20 = m√°s conservador)
        risk_reward_ratio: Ratio riesgo/recompensa (1.5 = m√°s conservador)
        atr_multiplier: Multiplicador para ATR (1.2 = m√°s conservador)
    
    Returns:
        DataFrame con columna 'target' a√±adida
    """
    print(f"üéØ Generando targets conservadores: lookforward {forward_candles} velas, R/R {risk_reward_ratio}")
    
    df = df.copy()
    df['target'] = 0
    
    try:
        # ‚úÖ USAR ATR DISPONIBLE DE FORMA ROBUSTA
        atr_column = None
        possible_atr_cols = ['atr', 'atr_14', 'atr_20', 'ATR']
        
        for col in possible_atr_cols:
            if col in df.columns and not df[col].isna().all():
                atr_column = col
                break
        
        if atr_column is None:
            print("‚ö†Ô∏è No se encontr√≥ columna ATR v√°lida, calculando ATR simple")
            # Calcular ATR b√°sico si no existe
            high_low = df['High'] - df['Low']
            high_close = abs(df['High'] - df['Close'].shift(1))
            low_close = abs(df['Low'] - df['Close'].shift(1))
            
            true_range = pd.DataFrame([high_low, high_close, low_close]).max()
            df['atr_temp'] = true_range.rolling(14).mean()
            atr_column = 'atr_temp'
        
        # ‚úÖ GENERAR TARGETS DE FORMA VECTORIZADA (M√ÅS EFICIENTE)
        highs = df['High'].values
        lows = df['Low'].values
        closes = df['Close'].values
        atrs = df[atr_column].values
        targets = np.zeros(len(df))
        
        print(f"üìä Procesando {len(df)} registros...")
        
        # Procesar en lotes para mayor eficiencia
        valid_indices = len(df) - forward_candles
        
        for i in range(valid_indices):
            if i % 1000 == 0:
                progress = (i / valid_indices) * 100
                print(f"‚è≥ Progreso: {progress:.1f}%")
            
            current_close = closes[i]
            current_atr = atrs[i]
            
            # Validar ATR
            if pd.isna(current_atr) or current_atr <= 0:
                continue
            
            # ‚úÖ C√ÅLCULO DE NIVELES CONSERVADORES
            stop_loss_price = current_close - (current_atr * atr_multiplier)
            take_profit_price = current_close + (current_atr * atr_multiplier * risk_reward_ratio)
            
            # Evaluar velas futuras
            future_highs = highs[i+1:i+forward_candles+1]
            future_lows = lows[i+1:i+forward_candles+1]
            
            # Verificar si se alcanza TP o SL primero
            tp_hit_indices = np.where(future_highs >= take_profit_price)[0]
            sl_hit_indices = np.where(future_lows <= stop_loss_price)[0]
            
            if len(tp_hit_indices) > 0 and len(sl_hit_indices) > 0:
                # Ambos golpeados, verificar cu√°l primero
                first_tp = tp_hit_indices[0]
                first_sl = sl_hit_indices[0]
                
                if first_tp < first_sl:
                    targets[i] = 1  # TP primero
                elif first_sl < first_tp:
                    targets[i] = -1  # SL primero
                # Si mismo √≠ndice, no asignar target (empate)
                
            elif len(tp_hit_indices) > 0:
                targets[i] = 1  # Solo TP golpeado
            elif len(sl_hit_indices) > 0:
                targets[i] = -1  # Solo SL golpeado
            # Si ninguno golpeado, target = 0 (no trade)
        
        df['target'] = targets
        
        # ‚úÖ ESTAD√çSTICAS DEL TARGET
        total_signals = len(df[df['target'] != 0])
        long_signals = len(df[df['target'] == 1])
        short_signals = len(df[df['target'] == -1])
        
        print(f"üìä Estad√≠sticas de targets:")
        print(f"   Total se√±ales: {total_signals} ({total_signals/len(df)*100:.1f}%)")
        print(f"   Se√±ales LONG: {long_signals} ({long_signals/total_signals*100:.1f}%)" if total_signals > 0 else "   Se√±ales LONG: 0")
        print(f"   Se√±ales SHORT: {short_signals} ({short_signals/total_signals*100:.1f}%)" if total_signals > 0 else "   Se√±ales SHORT: 0")
        
        # Limpiar columna temporal si se cre√≥
        if 'atr_temp' in df.columns:
            df.drop('atr_temp', axis=1, inplace=True)
        
        return df
        
    except Exception as e:
        print(f"‚ùå Error generando targets: {e}")
        import traceback
        traceback.print_exc()
        df['target'] = 0  # Fallback seguro
        return df

def detect_file_format(file_path):
    """
    ‚úÖ DETECTA AUTOM√ÅTICAMENTE EL FORMATO DE ARCHIVO
    
    Returns:
        dict: {'separator': str, 'encoding': str, 'decimal': str}
    """
    
    encodings_to_try = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
    separators_to_try = ['\t', ',', ';', '|']
    
    for encoding in encodings_to_try:
        for separator in separators_to_try:
            try:
                # Leer solo las primeras filas para detectar formato
                test_df = pd.read_csv(file_path, sep=separator, encoding=encoding, nrows=5)
                
                # Verificar que tenga columnas razonables
                if len(test_df.columns) >= 4:  # Al menos OHLC
                    # Verificar si las columnas parecen precios
                    numeric_cols = test_df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) >= 4:
                        return {
                            'separator': separator,
                            'encoding': encoding,
                            'decimal': '.'
                        }
                        
            except Exception:
                continue
    
    # Fallback por defecto
    return {'separator': '\t', 'encoding': 'utf-8', 'decimal': '.'}

def process_symbol(symbol, raw_data_dir, features_dir, target_params=None):
    """
    ‚úÖ PROCESADOR DE S√çMBOLO MEJORADO
    Procesa un s√≠mbolo completo: carga, a√±ade features, genera target y guarda
    
    Args:
        symbol: S√≠mbolo a procesar (ej: 'EURUSD')
        raw_data_dir: Directorio de datos raw
        features_dir: Directorio de destino para features
        target_params: Par√°metros para generaci√≥n de target
    """
    
    if target_params is None:
        target_params = {
            'forward_candles': 20,    # M√°s conservador
            'risk_reward_ratio': 1.5, # M√°s conservador
            'atr_multiplier': 1.2     # M√°s conservador
        }
    
    # ‚úÖ BUSCAR ARCHIVO CON DIFERENTES FORMATOS
    possible_files = [
        f"{symbol}_5m.csv",
        f"{symbol}_M5.csv", 
        f"{symbol}.csv",
        f"{symbol}_5min.csv",
        f"{symbol.lower()}_5m.csv"
    ]
    
    raw_file_path = None
    for filename in possible_files:
        potential_path = os.path.join(raw_data_dir, filename)
        if os.path.exists(potential_path):
            raw_file_path = potential_path
            break
    
    if raw_file_path is None:
        print(f"‚ùå No se encontr√≥ archivo de datos para {symbol}")
        print(f"   Buscados: {possible_files}")
        return False
    
    feature_file_path = os.path.join(features_dir, f"{symbol}_features.parquet")
    
    print(f"üîÑ Procesando {symbol}")
    print(f"üìÅ Archivo fuente: {os.path.basename(raw_file_path)}")
    
    try:
        # ‚úÖ DETECTAR FORMATO AUTOM√ÅTICAMENTE
        file_format = detect_file_format(raw_file_path)
        print(f"üìä Formato detectado: sep='{file_format['separator']}', encoding='{file_format['encoding']}'")
        
        # ‚úÖ CARGAR DATOS CON MANEJO ROBUSTO
        df = pd.read_csv(
            raw_file_path, 
            sep=file_format['separator'],
            encoding=file_format['encoding'],
            decimal=file_format['decimal']
        )
        
        print(f"üìä Datos cargados: {len(df)} registros, {len(df.columns)} columnas")
        print(f"üìà Columnas: {list(df.columns)}")
        
        # ‚úÖ VERIFICAR DATOS V√ÅLIDOS
        if len(df) < 100:
            print(f"‚ö†Ô∏è Datos insuficientes para {symbol}: {len(df)} registros")
            return False
        
        # Verificar columnas OHLC b√°sicas
        required_patterns = ['open', 'high', 'low', 'close']
        column_mapping = {}
        
        for pattern in required_patterns:
            found = False
            for col in df.columns:
                if pattern.lower() in col.lower():
                    column_mapping[col] = pattern.capitalize()
                    found = True
                    break
            
            if not found:
                print(f"‚ùå No se encontr√≥ columna para {pattern} en {symbol}")
                return False
        
        # Aplicar mapping si es necesario
        if column_mapping:
            df = df.rename(columns=column_mapping)
            print(f"üìä Columnas renombradas: {column_mapping}")
        
    except Exception as e:
        print(f"‚ùå Error cargando datos para {symbol}: {e}")
        return False
    
    try:
        # ‚úÖ CALCULAR FEATURES
        print("‚öôÔ∏è Calculando indicadores t√©cnicos...")
        df_features = add_all_features(df.copy())
        
        if df_features.empty:
            print(f"‚ùå No se pudieron calcular features para {symbol}")
            return False
        
        print(f"‚úÖ Features calculados: {len(df_features)} registros v√°lidos")
        
    except Exception as e:
        print(f"‚ùå Error calculando features para {symbol}: {e}")
        return False
    
    try:
        # ‚úÖ GENERAR TARGETS
        print("üéØ Generando targets...")
        df_final = generate_target(df_features.copy(), **target_params)
        
        print(f"‚úÖ Targets generados")
        
    except Exception as e:
        print(f"‚ùå Error generando targets para {symbol}: {e}")
        df_final = df_features  # Usar sin targets si falla
    
    try:
        # ‚úÖ GUARDAR FEATURES
        print(f"üíæ Guardando features en {feature_file_path}")
        df_final.to_parquet(feature_file_path, compression='snappy')
        
        # ‚úÖ VERIFICAR ARCHIVO GUARDADO
        file_size = os.path.getsize(feature_file_path) / (1024 * 1024)  # MB
        print(f"‚úÖ Archivo guardado: {file_size:.1f} MB")
        
        # ‚úÖ VERIFICAR FEATURES PARA OPTUNA
        print(f"üîç Verificando compatibilidad con OPTUNA:")
        
        # Probar algunos casos t√≠picos de par√°metros que puede sugerir Optuna
        optuna_test_cases = [
            ('ema_crossover', {'fast_period': 25, 'slow_period': 125}),
            ('rsi_pullback', {'rsi_period': 15, 'trend_ema_period': 150}),
            ('volatility_breakout', {'atr_period': 15}),
            ('multi_filter_scalper', {'ema_fast': 12, 'ema_mid': 30, 'rsi_len': 8, 'atr_len': 16})
        ]
        
        compatible_count = 0
        for strategy, params in optuna_test_cases:
            compatible, missing = verify_optuna_compatibility(df_final, strategy, params)
            status = "‚úÖ" if compatible else "‚ö†Ô∏è"
            print(f"   {status} {strategy} (test params)")
            if compatible:
                compatible_count += 1
            elif missing and len(missing) <= 3:
                print(f"       Faltantes: {missing}")
        
        compatibility_pct = (compatible_count / len(optuna_test_cases)) * 100
        print(f"   üìä Compatibilidad OPTUNA: {compatibility_pct:.0f}%")
        
        print(f"‚úÖ Procesamiento de {symbol} completado\n")
        return True
        
    except Exception as e:
        print(f"‚ùå Error guardando features para {symbol}: {e}")
        return False

def batch_process_symbols(raw_data_dir='data/raw', features_dir='data/features', target_params=None):
    """
    ‚úÖ PROCESAMIENTO EN LOTE DE TODOS LOS S√çMBOLOS
    
    Args:
        raw_data_dir: Directorio de datos raw
        features_dir: Directorio de destino
        target_params: Par√°metros para targets
    
    Returns:
        dict: Resultados del procesamiento
    """
    
    print("üöÄ GENERADOR DE FEATURES MEJORADO")
    print("=" * 60)
    
    # ‚úÖ CREAR DIRECTORIO DE FEATURES
    if not os.path.exists(features_dir):
        os.makedirs(features_dir)
        print(f"üìÅ Directorio creado: {features_dir}")
    
    # ‚úÖ BUSCAR ARCHIVOS DE DATOS
    if not os.path.exists(raw_data_dir):
        print(f"‚ùå Directorio de datos no encontrado: {raw_data_dir}")
        return {'success': False, 'error': 'Raw data directory not found'}
    
    print(f"üîç Buscando archivos en: {raw_data_dir}")
    
    all_files = os.listdir(raw_data_dir)
    csv_files = [f for f in all_files if f.endswith('.csv')]
    
    if not csv_files:
        print(f"‚ùå No se encontraron archivos .csv en {raw_data_dir}")
        return {'success': False, 'error': 'No CSV files found'}
    
    print(f"üìä Archivos encontrados: {len(csv_files)}")
    for f in csv_files:
        print(f"   üìÑ {f}")
    
    # ‚úÖ EXTRAER S√çMBOLOS DE NOMBRES DE ARCHIVO
    symbols_to_process = []
    
    for csv_file in csv_files:
        # Intentar extraer s√≠mbolo del nombre del archivo
        base_name = csv_file.replace('.csv', '')
        
        # Patrones comunes: SYMBOL_5m, SYMBOL_M5, SYMBOL
        possible_symbol = base_name.replace('_5m', '').replace('_M5', '').replace('_5min', '')
        
        if len(possible_symbol) >= 3:  # M√≠nimo 3 caracteres para s√≠mbolo
            symbols_to_process.append(possible_symbol.upper())
    
    # Eliminar duplicados
    symbols_to_process = list(set(symbols_to_process))
    
    if not symbols_to_process:
        print("‚ùå No se pudieron extraer s√≠mbolos v√°lidos de los archivos")
        return {'success': False, 'error': 'No valid symbols extracted'}
    
    print(f"\nüéØ S√≠mbolos a procesar: {symbols_to_process}")
    
    # ‚úÖ PAR√ÅMETROS CONSERVADORES PARA TARGETS
    if target_params is None:
        target_params = {
            'forward_candles': 20,       # 20 velas futuras (1.7 horas en M5) - m√°s conservador
            'risk_reward_ratio': 1.5,    # R/R 1:1.5 - m√°s conservador
            'atr_multiplier': 1.2        # 1.2x ATR para SL - m√°s conservador
        }
    
    print(f"\n‚öôÔ∏è Par√°metros conservadores de target:")
    for key, value in target_params.items():
        print(f"   {key}: {value}")
    
    # ‚úÖ PROCESAR S√çMBOLOS
    results = {
        'processed': [],
        'failed': [],
        'total_files': 0,
        'total_size_mb': 0
    }
    
    print(f"\nüîÑ Iniciando procesamiento...")
    
    for i, symbol in enumerate(symbols_to_process, 1):
        print(f"\n[{i}/{len(symbols_to_process)}] Procesando {symbol}")
        print("-" * 40)
        
        try:
            success = process_symbol(symbol, raw_data_dir, features_dir, target_params)
            
            if success:
                results['processed'].append(symbol)
                
                # Calcular tama√±o del archivo generado
                feature_file = os.path.join(features_dir, f"{symbol}_features.parquet")
                if os.path.exists(feature_file):
                    file_size = os.path.getsize(feature_file) / (1024 * 1024)
                    results['total_size_mb'] += file_size
                    results['total_files'] += 1
                
            else:
                results['failed'].append(symbol)
                
        except Exception as e:
            print(f"‚ùå Error procesando {symbol}: {e}")
            results['failed'].append(symbol)
    
    # ‚úÖ RESUMEN FINAL
    print("\n" + "=" * 60)
    print("üìä RESUMEN DE PROCESAMIENTO")
    print("=" * 60)
    
    print(f"‚úÖ Procesados exitosamente: {len(results['processed'])}")
    for symbol in results['processed']:
        print(f"   üìà {symbol}")
    
    if results['failed']:
        print(f"\n‚ùå Fallidos: {len(results['failed'])}")
        for symbol in results['failed']:
            print(f"   üìâ {symbol}")
    
    print(f"\nüìä Estad√≠sticas:")
    print(f"   üìÅ Archivos generados: {results['total_files']}")
    print(f"   üíæ Tama√±o total: {results['total_size_mb']:.1f} MB")
    print(f"   üìà Tasa de √©xito: {len(results['processed'])/len(symbols_to_process)*100:.1f}%")
    
    # ‚úÖ VERIFICAR COMPATIBILIDAD CON ESTRATEGIAS
    if results['processed']:
        print(f"\nüîç Verificando compatibilidad con estrategias:")
        
        # Cargar primer archivo para verificar
        test_symbol = results['processed'][0]
        test_file = os.path.join(features_dir, f"{test_symbol}_features.parquet")
        
        try:
            test_df = pd.read_parquet(test_file)
            
            # ‚úÖ PROBAR COMPATIBILIDAD CON OPTUNA
            print(f"üîç Verificando compatibilidad OPTUNA con archivo de ejemplo:")
            
            # Usar la funci√≥n de testing
            compatible = test_optuna_compatibility() if 'test_optuna_compatibility' in globals() else True
            
            if compatible:
                print(f"   ‚úÖ Totalmente compatible con OPTUNA")
            else:
                print(f"   ‚ö†Ô∏è Puede haber incompatibilidades menores")
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error verificando compatibilidad: {e}")
    
    results['success'] = len(results['processed']) > 0
    
    if results['success']:
        print(f"\nüéâ Procesamiento completado exitosamente!")
        print(f"üöÄ Ejecuta 'python run_optimization.py' para optimizar estrategias")
    else:
        print(f"\n‚ùå Procesamiento fall√≥ para todos los s√≠mbolos")
    
    return results

def main():
    """Funci√≥n principal para ejecuci√≥n directa"""
    
    import argparse
    
    parser = argparse.ArgumentParser(description="Generador de Features para Trading Bot")
    parser.add_argument('--raw-dir', default='data/raw', help='Directorio de datos raw')
    parser.add_argument('--features-dir', default='data/features', help='Directorio de features')
    parser.add_argument('--forward-candles', type=int, default=20, help='Velas futuras para target')
    parser.add_argument('--risk-reward', type=float, default=1.5, help='Risk/Reward ratio')
    parser.add_argument('--atr-multiplier', type=float, default=1.2, help='Multiplicador ATR')
    
    args = parser.parse_args()
    
    # Par√°metros personalizados
    target_params = {
        'forward_candles': args.forward_candles,
        'risk_reward_ratio': args.risk_reward,
        'atr_multiplier': args.atr_multiplier
    }
    
    # Ejecutar procesamiento
    results = batch_process_symbols(
        raw_data_dir=args.raw_dir,
        features_dir=args.features_dir,
        target_params=target_params
    )
    
    # Exit code basado en resultados
    if results['success']:
        sys.exit(0)
    else:
        sys.exit(1)

# ‚úÖ EJECUCI√ìN DIRECTA O COMO M√ìDULO
if __name__ == '__main__':
    # Si se ejecuta directamente con argumentos, usar argparse
    if len(sys.argv) > 1:
        main()
    else:
        # Ejecuci√≥n simple sin argumentos
        results = batch_process_symbols()
        
        if not results['success']:
            print("\nüí° AYUDA:")
            print("1. Aseg√∫rate de tener archivos CSV en 'data/raw/'")
            print("2. Los archivos deben tener formato: SYMBOL_5m.csv")
            print("3. Deben contener columnas OHLC (Open, High, Low, Close)")
            
            # Mostrar ejemplo de uso con argumentos
            print(f"\nüìñ Uso avanzado:")
            print(f"python {__file__} --raw-dir data/raw --features-dir data/features --forward-candles 20 --risk-reward 1.5")

# ‚úÖ FUNCI√ìN DE TESTING
def test_generate_features():
    """Funci√≥n para probar el generador completo"""
    
    print("üß™ TESTING GENERATE FEATURES")
    print("=" * 50)
    
    # Crear datos de prueba
    test_dir = "test_data"
    raw_dir = os.path.join(test_dir, "raw")
    features_dir = os.path.join(test_dir, "features")
    
    # Crear directorios
    os.makedirs(raw_dir, exist_ok=True)
    os.makedirs(features_dir, exist_ok=True)
    
    try:
        # Generar datos sint√©ticos EURUSD
        np.random.seed(42)
        n_samples = 2000
        
        dates = pd.date_range(start='2023-01-01', periods=n_samples, freq='5T')
        base_price = 1.1000
        returns = np.random.normal(0, 0.0008, n_samples)
        prices = base_price * np.exp(np.cumsum(returns))
        
        # Crear OHLC realista
        df_test = pd.DataFrame({
            'Time': dates,
            'Open': prices,
            'High': prices * (1 + np.random.uniform(0, 0.002, n_samples)),
            'Low': prices * (1 - np.random.uniform(0, 0.002, n_samples)),
            'Close': prices * (1 + np.random.normal(0, 0.001, n_samples)),
            'Volume': np.random.randint(1000, 10000, n_samples)
        })
        
        # Ajustar para que High >= max(Open, Close) y Low <= min(Open, Close)
        df_test['High'] = np.maximum(df_test['High'], df_test[['Open', 'Close']].max(axis=1))
        df_test['Low'] = np.minimum(df_test['Low'], df_test[['Open', 'Close']].min(axis=1))
        
        # Guardar archivo de prueba
        test_file = os.path.join(raw_dir, "EURUSD_5m.csv")
        df_test.to_csv(test_file, sep='\t', index=False)
        
        print(f"‚úÖ Archivo de prueba creado: {test_file}")
        print(f"üìä Datos: {len(df_test)} registros")
        
        # Procesar archivo
        results = batch_process_symbols(raw_dir, features_dir)
        
        if results['success']:
            print(f"‚úÖ Testing exitoso!")
            
            # Verificar archivo generado
            feature_file = os.path.join(features_dir, "EURUSD_features.parquet")
            if os.path.exists(feature_file):
                df_features = pd.read_parquet(feature_file)
                print(f"üìä Features generados: {len(df_features)} registros, {len(df_features.columns)} columnas")
                
                # Verificar targets
                if 'target' in df_features.columns:
                    target_stats = df_features['target'].value_counts()
                    print(f"üéØ Target distribution: {dict(target_stats)}")
        else:
            print(f"‚ùå Testing fall√≥")
        
    except Exception as e:
        print(f"‚ùå Error en testing: {e}")
    
    finally:
        # Limpiar archivos de prueba
        try:
            import shutil
            if os.path.exists(test_dir):
                shutil.rmtree(test_dir)
                print(f"üßπ Archivos de prueba eliminados")
        except:
            pass

if __name__ == "__main__" and len(sys.argv) == 1:
    # Solo ejecutar testing si no hay argumentos
    print("üîß Ejecutando en modo testing...")
    test_generate_features()